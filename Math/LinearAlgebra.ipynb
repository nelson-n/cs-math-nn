{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sections\n",
    "* Linear Algebra Notation\n",
    "* Central Theory of Linear Algebra\n",
    "* Vector Basics\n",
    "* Matrix Basics\n",
    "* Linear Transformations\n",
    "* Span and Linear Independence\n",
    "* Dot Product\n",
    "* Cross Product\n",
    "* Determinants\n",
    "* Matrix Multiplication\n",
    "* Systems of Linear Equations\n",
    "* Eigenvalues, Eigenvectors\n",
    "* Covariance Matrix\n",
    "* Spectral Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Notation\n",
    "\n",
    "$i_{th}, j_{th}$: Used to refer to an index of a matrix $[i, j]$ or to whole rows $i$ or whole columns $j$.\n",
    "\n",
    "$\\mathbf{A}^T$: The transpose of matrix $\\mathbf{A}$\n",
    "\n",
    "$\\text{det}(\\mathbf{A})$: The determinant of matrix $\\mathbf{A}$\n",
    "\n",
    "$\\text{rank}(\\mathbf{A})$: The rank of matrix $\\mathbf{A}$\n",
    "\n",
    "$\\text{tr}(\\mathbf{A})$: The trace of matrix $\\mathbf{A}$\n",
    "\n",
    "$\\mathbf{A}^{-1}$: The inverse of matrix $\\mathbf{A}$\n",
    "\n",
    "$\\mathbf{Ax} = \\mathbf{b}$: A system of linear equations where $\\mathbf{A}$ is a matrix, $\\mathbf{x}$ is a column vector of variables, and $\\mathbf{b}$ is a column vector of constants\n",
    "\n",
    "$\\mathbf{x} \\cdot \\mathbf{y}$ or $\\langle x, y \\rangle$: The inner product (dot product) of vectors $\\mathbf{x}$ and $\\mathbf{y}$, takes in two vectors and returns a scalar\n",
    "\n",
    "$\\mathbf{A} \\times \\mathbf{B}$: The cross product of matrices $\\mathbf{A}$ and $\\mathbf{B}$, takes in two vectors and returns a vector\n",
    "\n",
    "$\\mathbf{x} \\otimes \\mathbf{y}$: The outer product of vectors $\\mathbf{x}$ and $\\mathbf{y}$, takes in two vectors and returns a matrix\n",
    "\n",
    "$|\\mathbf{x}|$: The norm (length) of column vector $\\mathbf{x}$\n",
    "\n",
    "$\\mathbf{I}$: The identity matrix\n",
    "\n",
    "$\\mathbf{0}$: The zero matrix\n",
    "\n",
    "$\\text{span}({\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n})$: The span of column vectors $\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n$\n",
    "\n",
    "$\\text{null}(\\mathbf{A})$: The null space of matrix $\\mathbf{A}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Theory of Linear Algebra\n",
    "* All of linear algebra can be completed with two operations: vector addition and vector multiplication.\n",
    "* All vectors can be represented as scalar multiples of the two basis vectors *i* and *j*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Int64}:\n",
       "  8\n",
       " 12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "# Basis vectors.\n",
    "i = [1, 0]\n",
    "j = [0, 1]\n",
    "\n",
    "# Vector addition.\n",
    "[1, 2] + [3, 4]\n",
    "\n",
    "# Vector scaling.\n",
    "[4, 6] * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Basics\n",
    "* The magnitude (length) of a vector is calculated as: $||v||_2 = \\sqrt{v_1^2 + v_2^2 + ... + v_n^2}$\n",
    "* The magnitude is also called the L2 norm or Euclidean norm: $||v||_2 = (\\sum^n_{i=1}|x_i|^2)^{\\frac{1}{2}}$\n",
    "* We can also calculate the L1 norm or Manhattan norm: $||v||_1 = \\sum^n_{i=1}|x_i|$\n",
    "    * This can be thought of in 2D as drawing a line along the x-axis and then a line along the y-axis to the point. The sum of the lengths of the lines is the L1 norm.\n",
    "* The angle between two vectors is calculated as: $\\theta = cos^{-1}(\\frac{v \\cdot w}{||v|| \\cdot ||w||})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length calculated by hand: 4.58257569495584\n",
      "Vector length calculated by norm(): 4.58257569495584\n",
      "Angle between A and B: 69.81620322790722\n"
     ]
    }
   ],
   "source": [
    "A = [2, 1, 4]\n",
    "B = [1, 3, 0]\n",
    "\n",
    "# Calculating vector length by hand.\n",
    "v = sqrt(A[1]^2 + A[2]^2 + A[3]^2)\n",
    "println(\"Vector length calculated by hand: $v\")\n",
    "\n",
    "# Calculating vector length using norm() in Julia.\n",
    "v = norm(A)\n",
    "println(\"Vector length calculated by norm(): $v\")\n",
    "\n",
    "# Calculating the angle between A and B.\n",
    "Θ = acos(dot(A, B) / (norm(A) * norm(B))) # Outputted in radians, we could use acosd() to output in degrees.\n",
    "Θ = Θ * 180 / pi # Converting to degrees.\n",
    "println(\"Angle between A and B: $Θ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Basics\n",
    "* Matrix transpose flips a matrix over its diagonal (switches the row and column indices of a matrix).\n",
    "    * This is formally defined as: $(A^T)_{ij} = A_{ji}$\n",
    "* A square matrix whose transpose is equal to itself is called a symmetric matrix: $A = A^T$\n",
    "* A square matrix whose transpose is equal to its inverse is called an orthogonal matrix: $A^{-1} = A^T$\n",
    "* The identity matrix $I$ is a square matrix with ones on the diagonal and zeros elsewhere.\n",
    "* An invertible matrix is a matrix that when multiplied by the inverse of that matrix, equals the identity matrix: $A^{-1}A = I$\n",
    "    * Because the identity matrix $I$ is square, invertible matrices must be square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Matrix: [2 1 4; 3 3 1]\n",
      "Transposed Base Matrix: [2 3; 1 3; 4 1]\n",
      "Identity Matrix: [1 0 0; 0 1 0; 0 0 1]\n",
      "Invertible Matrix A * inv(A): [1.0 0.0; 0.0 1.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = [2 1 4; 3 3 1]\n",
    "\n",
    "# Transpose of a matrix.\n",
    "At = A'\n",
    "println(\"Base Matrix: $A\")\n",
    "println(\"Transposed Base Matrix: $At\")\n",
    "\n",
    "# The multiplication of a matrix with its transpose yields a symmetric matrix.\n",
    "# This is proven by showing that A*A' is its own transpose.\n",
    "(A*A')' == (A')'*A' == A*A'\n",
    "\n",
    "# Symmetric matrix example.\n",
    "S = [1 2 3; 2 4 5; 3 5 6]\n",
    "S == S'\n",
    "\n",
    "# The identity matrix.\n",
    "i = [1 0 0; 0 1 0; 0 0 1]\n",
    "println(\"Identity Matrix: $i\")\n",
    "\n",
    "# Orthogonal matrix example.\n",
    "# The inverse of an orthogonal matrix is its transpose.\n",
    "# This is proven by showing that A*A' is the identity matrix.\n",
    "A = [0 1; -1 0]\n",
    "inv(A) == A'\n",
    "\n",
    "# Invertible matrix example.\n",
    "out = A*inv(A)\n",
    "println(\"Invertible Matrix A * inv(A): $out\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Transformations\n",
    "* Linear transformation is the process of multiplying vectors by a matrix in order to transform them into new vectors.\n",
    "* Linear transformation can be thought of as a way to move around space. Every time that you see a matrix you can interpret it as a transformation of space.\n",
    "    * For example, rotating the standard 2D grid by 45 degrees. \n",
    "* The linear transformation of any vector can be described/calculated in terms of how the basis vectors *i* and *j* are transformed by the linear transformation.\n",
    "* Matrices are the language that describes linear transformation.\n",
    "    * Columns of the matrix represent how the basis vectors are transformed, they can be thought of as the coordinates of the transformed basis vectors.\n",
    "    * You can transform vectors between dimensions with nonsquare matrices.\n",
    "* Types of linear transformations: rotate, reflect, scale, shear, project.\n",
    "* Matrix multiplication stretches or squeezes the size of the grid, this degree of stretch/squeeze is encoded in the determinant.\n",
    "    * This stretch/squeeze can be represented by how much area the basis vectors take after transformation (by default 1i * 1j = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base vector: [2, 2]\n",
      "Reflection in the y-axis: [-2, 2]\n",
      "Reflection in the x-axis: [2, -2]\n",
      "Horizontal expansion: [4, 2]\n",
      "Horizontal sheer: [10, 2]\n",
      "Transformation from 2d to 3d w/ nonsquare matrix: [6, 14, 22]\n"
     ]
    }
   ],
   "source": [
    "vec = [2, 2]\n",
    "println(\"Base vector: $vec\")\n",
    "\n",
    "# Reflection in the y-axis.\n",
    "A = [-1 0; 0 1]\n",
    "out = A * vec\n",
    "println(\"Reflection in the y-axis: $out\")\n",
    "\n",
    "# Reflection in the x-axis.\n",
    "A = [1 0; 0 -1]\n",
    "out = A * vec\n",
    "println(\"Reflection in the x-axis: $out\")\n",
    "\n",
    "# Horizontal expansion.\n",
    "A = [2 0; 0 1]\n",
    "out = A * vec\n",
    "println(\"Horizontal expansion: $out\")\n",
    "\n",
    "# Horizontal shear.\n",
    "A = [1 4; 0 1]\n",
    "out = A * vec\n",
    "println(\"Horizontal sheer: $out\")\n",
    "\n",
    "# Transforming a vector from 2d to 3d space with a nonsquare matrix.\n",
    "A = [1 2; 3 4; 5 6]\n",
    "out = A * vec\n",
    "println(\"Transformation from 2d to 3d w/ nonsquare matrix: $out\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span and Linear Independence\n",
    "* **Vector Space** = Set of vectors {v1, v2, ..., vn} along with two operations, vector addition and scalar multiplication.\n",
    "* **Span** = Given a set of vectors {v1, v2, ..., vn} in a vector space V, the span is the set of all linear combinations of the vectors. \n",
    "    * The span is itself a vector space that represents all possible linear combinations of the given vectors.\n",
    "    * The dimension of the span equals the number of vectors in the vector space unless two of the vectors are linearly dependent (fall on the same line).\n",
    "* **Spanning Set** = Set of vectors that can be combined linearly to produce any vector in the vector space. A set of vectors that spans the entire vector space.\n",
    "* **Linear Independence** = A set of vectors is linearly independent if no vector in the set can be expressed as a linear combination of other vectors in the set.\n",
    "* **Basis** = The basis for a vector space is the set of linearly independent vectors that can be used to represent any vector in a vector space V.\n",
    "    * A vector space may have many different bases, but all bases have the same number of vectors, which is the dimension of the vector space.\n",
    "    * For example, the basis of 3D space is 3.\n",
    "* **Dimension** = Number of vectors in the basis.\n",
    "    * The dimension of any span equals the number of vectors in the span, unless two of the vectors are linearly dependent (fall on the same line).\n",
    "* **Rank (Matrix Rank)** = Number of dimensions in the output of a transformation.\n",
    "    * A linear transformation that transforms space to a line has rank 1.\n",
    "        * Note that in a Rank-1 matrix all rows and columns are multiples of each other, this is what transforms space to a line.\n",
    "    * A linear transformation that transforms space to a plane has rank 2.\n",
    "* **Column Space** = Span of the vectors that make up all columns in a matrix.\n",
    "* **Row Space** = Span of the vectors that make up all rows in a matrix.\n",
    "* **Null Space** = All possible solutions to the equation *Ax* = 0.\n",
    "    * All vectors *x* that become null (land on the zero vector) when transformed by *A*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Ax round to 0 (e-16): [-4.440892098500626e-16; -8.881784197001252e-16;;]\n"
     ]
    }
   ],
   "source": [
    "# Any vector in the span of two vectors can be expressed as a linear combination\n",
    "# of the two vectors. For example, [2, 3, 0] is in the span of [1, 0, 0] and [0, 1, 0].\n",
    "[2, 3, 0] == 2 * [1, 0, 0] + 3 * [0, 1, 0]\n",
    "\n",
    "# The basis for 3D space i, j, k that can be used to represent any vector in 3D space.\n",
    "# Because there are 3 vectors, the dimension of the space is 3.\n",
    "i = [1, 0, 0]\n",
    "j = [0, 1, 0]\n",
    "k = [0, 0, 1]\n",
    "\n",
    "# Calculating the nullspace of A and then calculating Ax = 0.\n",
    "A = [1 2; 3 4; 5 6]\n",
    "x = nullspace(A')\n",
    "out = A'x\n",
    "println(\"Output of Ax round to 0 (e-16): $out\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot Product \n",
    "* Also referred to as the *inner product* $\\langle x, y \\rangle$.\n",
    "* **Dot Product** = Length vector A * length vector B * cos(angle between the vectors).\n",
    "    * $A \\cdot B = ||A|| \\times ||B|| \\times \\cos(\\theta)$\n",
    "    * Geometrically, the dot product is equivalent to projecting one vector onto another, taking the length of the projection, and multiplying it by the length of the other vector.\n",
    "    * When the vectors point in the same direction, the projection and thus the dot product are positive.\n",
    "    * When the vectors point in opposite directions the dot product is negative.\n",
    "    * When the vectors are perpendicular (orthogonal), the projection onto the other vector has a length of 0, thus the dot product is 0.\n",
    "    * The dot product of a vector with itself is equal to the square of its magnitude (length).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product via sum of products: 22\n",
      "Dot product via cos(Θ) approach: 22.0\n",
      "Dot product via dot() function: 22\n",
      "Dot product of orthogonal vectors: 0\n"
     ]
    }
   ],
   "source": [
    "# Dot Product\n",
    "A = [1, 4, 2]\n",
    "B = [2, 4, 2]\n",
    "\n",
    "# The dot product is calculated as the sum of the products of the corresponding\n",
    "# entries of the two vectors. \n",
    "out = (1 * 2) + (4 * 4) + (2 * 2)\n",
    "println(\"Dot product via sum of products: $out\")\n",
    "\n",
    "# Calculating the dot product as ||A|| * ||B|| * cos(theta).\n",
    "Θ = acosd(dot(A, B) / (norm(A) * norm(B))) \n",
    "out = norm(A) * norm(B) * cosd(Θ)\n",
    "println(\"Dot product via cos(Θ) approach: $out\")\n",
    "\n",
    "# Dot product via the dot() function.\n",
    "out = dot(A, B)\n",
    "println(\"Dot product via dot() function: $out\")\n",
    "\n",
    "# The dot product of a vector with itself is equal to the square of its magnitude\n",
    "# (length). The length of the vector [2, 2] is 2.82.\n",
    "C = [2, 2]\n",
    "sqrt(dot(C, C)) == norm(C)\n",
    "\n",
    "# The dot product of orthogonal vectors is 0.\n",
    "A = [10, 0]\n",
    "B = [0, 10]\n",
    "out = dot(A, B)\n",
    "println(\"Dot product of orthogonal vectors: $out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cross Product\n",
    "* **Cross Product** = Takes two vectors and produces a third vector that is orthogonal to both of the original vectors.\n",
    "    * The cross product, $v \\times w$ is the area of the parallelogram formed by the two vectors\n",
    "    * The cross product of a vector with itself is 0. \n",
    "    * The cross product is simply the determinant of the matrix created by combining vectors v and w, this is because the determinant is a measure of how much the area changes as a result of transformation, and this area is the area of the parallelogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross product calculated by hand: [0, 2, -4]\n",
      "Cross product calculated using cross(): [0, 2, -4]\n",
      "dot(cross(A, B), A): 0\n",
      "dot(cross(A, B), B): 0\n",
      "Cross product of a vector with itself: [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = [1, 4, 2]\n",
    "B = [2, 4, 2]\n",
    "\n",
    "# Formula for calculating the cross product.\n",
    "out = [A[2] * B[3] - A[3] * B[2], A[3] * B[1] - A[1] * B[3], A[1] * B[2] - A[2] * B[1]]\n",
    "println(\"Cross product calculated by hand: $out\")\n",
    "\n",
    "# Cross product using the cross function.\n",
    "cross(A, B)\n",
    "println(\"Cross product calculated using cross(): $out\")\n",
    "\n",
    "# The cross product of A and B is orthogonal to both A and B.\n",
    "out = dot(cross(A, B), A)\n",
    "println(\"dot(cross(A, B), A): $out\")\n",
    "\n",
    "out = dot(cross(A, B), B)\n",
    "println(\"dot(cross(A, B), B): $out\")\n",
    "\n",
    "# The cross product of a vector with itself is 0.\n",
    "out = cross(A, A)\n",
    "println(\"Cross product of a vector with itself: $out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Product\n",
    "* Also referred to as the *tensor product* $\\mathbf{x} \\otimes \\mathbf{y}$\n",
    "* **Outer Product** = given an *n* dimensional vector *u* and a *m* dimensional vector *v* returns an *n x m* matrix.\n",
    "\n",
    "$$u \\otimes v = \\begin{bmatrix} u_1v_1 & u_1v_2 & \\cdots & u_1v_m \\\\ u_2v_1 & u_2v_2 & \\cdots & u_2v_m \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ u_nv_1 & u_nv_2 & \\cdots & u_nv_m \\end{bmatrix}$$\n",
    "\n",
    "* The output matrix provides information about the pairwise relationship between the elements of the two input vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Int64}:\n",
       "  4   5   6\n",
       "  8  10  12\n",
       " 12  15  18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Outer product example in julia.\n",
    "A = [1, 2, 3]\n",
    "B = [4, 5, 6]\n",
    "A * B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinants\n",
    "* **Determinant** = Scalar value that measures how much the area of a vector space changes as a result of a linear transformation with a square matrix.\n",
    "    * The determinant can be thought of as the \"area scaling factor\" that a linear transformation using the matrix would induce.\n",
    "    * Can only be computed for square matrices.\n",
    "    * Negative determinants represent a flip of orientation.\n",
    "    * When the determinant is 0, *i* has been transformed so it is a linear combination of *j* (on the same line). The area of the grid is now 0, thus the determinant is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x2 determinant calculated by hand: -2\n",
      "2x2 determinant calculated by via det(): -2.0\n",
      "Determinant of a horizontal expansion by 2: 2.0\n",
      "Determinant of A calculated via Laplace expansion: 104.0\n",
      "Determinant of A calculated using eigenvalues: 103.99999999999994\n",
      "Determinant of a linearly dependent matrix: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The determinant of a 2x2 matrix [a b; c d] is given by the formula ad - bc.\n",
    "A = [1 2; 3 4]\n",
    "out = 1 * 4 - 2 * 3\n",
    "println(\"2x2 determinant calculated by hand: $out\")\n",
    "\n",
    "out = det(A)\n",
    "println(\"2x2 determinant calculated by via det(): $out\")\n",
    "\n",
    "# The determinant of a horizontal expansion by a factor of 2 is simply 2. \n",
    "A = [2 0; 0 1]\n",
    "out = det(A)\n",
    "println(\"Determinant of a horizontal expansion by 2: $out\")\n",
    "\n",
    "# The determinant of larger matrices can be manually calculated using Laplace expansion.\n",
    "A = [1 2 3; 4 1 6; 7 8 1]\n",
    "out = det(A)\n",
    "println(\"Determinant of A calculated via Laplace expansion: $out\")\n",
    "\n",
    "# The determinant of a matrix is also the product of the eigenvalues of the matrix.\n",
    "vals = eigvals(A)\n",
    "out = vals[1] * vals[2] * vals[3]\n",
    "println(\"Determinant of A calculated using eigenvalues: $out\")\n",
    "\n",
    "# When the columns are linearly dependent, the determinant is 0.\n",
    "A = [1 2 3; 2 4 6; 4 8 12]\n",
    "out = det(A)\n",
    "println(\"Determinant of a linearly dependent matrix: $out\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "* Matrix multiplication is simply a composition of linear transformations. \n",
    "    * Each matrix represents a linear transformation, and the multiplication of both matrices tells you what happens to the basis vectors when both linear transformations are applied.\n",
    "    * For example, two matrices may represent a grid being rotated 90 degrees and then being sheared.\n",
    "    * The order in which matrices are multiplied matters, a 90 degree rotation followed by a shear results in a different matrix than a shear followed by a 90 degree rotation.\n",
    "    * Matrix multiplication is not commutative, $AB \\neq BA$.\n",
    "* The inner dimensions of the matrices must match, the outer dimensions of the resulting matrix will be the outer dimensions of the two input matrices.\n",
    "    * $A_{m \\times n} \\times B_{n \\times p} = C_{m \\times p}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of composition of linear transformations: [-4, 2]\n"
     ]
    }
   ],
   "source": [
    "vec = [2, 2]\n",
    "\n",
    "# Reflect over the y-axis and then stretch in the horizontal direction.\n",
    "A = [-1 0; 0 1]\n",
    "B = [2 0; 0 1]\n",
    "composition = A * B\n",
    "out = composition * vec \n",
    "println(\"Result of composition of linear transformations: $out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of Linear Equations\n",
    "* The geometric interpretation of solving the fundamental equation *Ax* = *b* is the following: We are looking for a matrix *A* that transforms vector *x* into vector *b*.\n",
    "    * *Ax* = *b* is solving for which original vector *x* is stretched to *b* when the matrix *A* transforms the space.\n",
    "* *A* is a coefficient matrix, *x* is the vector of variables that we want to solve for, and *b* is vector of constants on the right hand side of the equation. \n",
    "* If the matrix *A* is invertible (its determinant does not equal 0), then we can solve for *x* using the form: $x = A^{-1}b$.\n",
    "    * If det(A) does not equal 0 than the columns of A are linearly dependent and the transformation using A does not squish all space into a lower dimension, thus there is only one vector *x* that can be transformed into vector *b* via *A*.\n",
    "    * If det(A) = 0 then the transformation from *A* squishes space into a lower dimension and it is likely that there is no solution. There will only be a solution if *b* falls upon the lower dimension line/plane/etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix: [1 2 2; 3 -2 1; 2 1 -1]\n",
      "b output: [5, -6, 1]\n",
      "b[1] estimate: 5.000000000000001\n",
      "b[2] estimate: -6.0\n",
      "b[3] estimate: 1.0000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Take the system of equations:\n",
    "# x + 2y + 2z = 5\n",
    "# 3x - 2y + z = -6\n",
    "# 2x + y - z = 1\n",
    "\n",
    "# This can be represented as:\n",
    "A = [1 2 2; 3 -2 1; 2 1 -1]\n",
    "# x = [x, y, z]\n",
    "b = [5, -6, 1]\n",
    "\n",
    "println(\"A matrix: $A\")\n",
    "println(\"b output: $b\")\n",
    "\n",
    "# The vector x is solved for by multiplying the inverse of A by b.\n",
    "coefs = inv(A) * b\n",
    "\n",
    "# Check outputs.\n",
    "out = (1 * coefs[1]) + (2 * coefs[2]) + (2 * coefs[3])\n",
    "println(\"b[1] estimate: $out\")\n",
    "\n",
    "out = (3 * coefs[1]) + (-2 * coefs[2]) + (1 * coefs[3])\n",
    "println(\"b[2] estimate: $out\")\n",
    "\n",
    "out = (2 * coefs[1]) + (1 * coefs[2]) + (-1 * coefs[3])\n",
    "println(\"b[3] estimate: $out\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors, Eigenvalues\n",
    "* **Eigenvector** = Vector that does not get knocked off its span (the 1D line that houses all scalar multiples of the vector) when a matrix transformation *A* is applied to the vector.\n",
    "    * $A * v = \\lambda * v$ where *v* is the eigenvector and *lambda* is the eigenvalue.\n",
    "    * $\\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}$ * $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ = 2 * $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$\n",
    "* **Eigenvalue** = The factor with which the vector is stretched or squished during the transformation. \n",
    "* Eigenvectors can be used to scale down large matrices. If $M$ is a large matrix, we can look for a vector $o$ and a scalar $n$ which can be used to generate $M$.\n",
    "    * Then one only needs the eigenvector and eigenvalue ($o$ and $n$) to generate the matrix.\n",
    "    * $M \\times o = o \\times n$ where $o$ is the eigenvector and $n$ is the eigenvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [2 0; 0 3]\n",
    "\n",
    "# Find the eigenvectors and eigenvalues of A.\n",
    "out = eigvecs(A)\n",
    "println(\"eigenvectors of A: $out\")\n",
    "\n",
    "out = eigvals(A)\n",
    "println(\"eigenvalues of A: $out\")\n",
    "\n",
    "# For both eigenvectors and eigenvalues, A * eigvec = eigvec * eigval.\n",
    "A * eigvecs(A)[:, 1] == eigvecs(A)[:, 1] * eigvals(A)[1]\n",
    "A * eigvecs(A)[:, 2] == eigvecs(A)[:, 2] * eigvals(A)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Matrix\n",
    "\n",
    "* **Covariance Matrix** = Matrix of the form $X^TX$, calculates the covariance between each pair of variables in a dataset.\n",
    "* Recall, covariance measures the direction and degree to which two variables vary together: $cov(X, Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n-1}$\n",
    "* The $(i, j)^{th}$ entry of $X^TX$ tells us how similar the $i^{th}$ feature (row) is to the $j^{th}$ feature (column).\n",
    "* $X^TX$ is a symmetric matrix, the diagonal entries are the variance of each feature, and the off-diagonal entries are the covariance between each pair of features.\n",
    "* **Fact**: All eigenvalues of symmetric matrices are $>= 0$.\n",
    "* To calculate the covariance matrix:\n",
    "    * 1) Center the data by subtracting the mean from each feature.\n",
    "    * 2) Calculate the covariance matrix using an outer product: $X^TX$.\n",
    "    * 3) Divide the covariance matrix by the number of observations (rows/cols) minus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "using Statistics\n",
    "\n",
    "# Calculate covariance matrix using built-in cov() function.\n",
    "A = [2 2 3; 4 2 6; 2 8 14]\n",
    "cov(A)\n",
    "\n",
    "# Calculate covariance matrix by hand:\n",
    "\n",
    "# Center the data by subtracting the mean from each column.\n",
    "Adm = A .- mean(A, dims=1)\n",
    "\n",
    "# Calculate the covariance matrix.\n",
    "CovMat = (Adm' * Adm) / (size(A)[1] - 1)\n",
    "\n",
    "# Show that the diaganol entries are the variance of each feature.\n",
    "var(A[:, 1]) # Variance of the first column.\n",
    "CovMat[1, 1] # Diagonal entry of the covariance matrix.\n",
    "\n",
    "var(A[:, 2]) # Variance of the first column.\n",
    "CovMat[2, 2] # Diagonal entry of the covariance matrix.\n",
    "\n",
    "# Show that the off-diagonal entries are the covariance between features.\n",
    "cov(A[:, 1], A[:, 2]) # Covariance between the first and second columns.\n",
    "CovMat[1, 2] # Off-diagonal entry of the covariance matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Theorem\n",
    "* **Spectral Theorem**: Every symmetric matrix has an eigen decomposition: $A = Q \\cdot D \\cdot Q^T$.\n",
    "    * Where $A$ is a symmetric matrix.\n",
    "    * $Q$ is an orthogonal matrix of eigenvectors, each column is an eigenvector.\n",
    "    * $D$ is a diagonal matix where the diagonal entries are the eigenvalues of $A$.\n",
    "    * $Q^T$ is the transpose of $Q$.\n",
    "* The eigen decomposition of $A$ tells us that $A$ is \"almost\" a diagonal matrix (the eigenvalues are on the diagonal of $D$).\n",
    "    * The eigenvectors in $Q$ tell us how to rotate the basis vectors to make $A$ diagonal.\n",
    "    * The eigenvalues in $D$ tell us how much to stretch the basis vectors to make $A$ diagonal.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spectral Theorem example in Julia.\n",
    "A = [2 0; 0 3]\n",
    "\n",
    "Q = eigvecs(A)\n",
    "D = Diagonal(eigvals(A))\n",
    "\n",
    "Q * D * Q' == A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
